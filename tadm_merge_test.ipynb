{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Step 1 Merge Together TADM data from Folder\n",
    "\n",
    "files = ['Example/'+x for x in os.listdir('Example')]\n",
    "files\n",
    "curvefile = [x for x in files if 'Curves' in x][0]\n",
    "curves_df = pd.read_csv(curvefile).set_index(['CurveID', 'Sheet'])\n",
    "pressurevalues = {}\n",
    "##Get unique pressurevalues\n",
    "for pressurevalue in curves_df.index.unique(1):\n",
    "    df = pd.read_csv([x for x in files if pressurevalue in x][0])\n",
    "    df = df.set_index('Time').transpose()\n",
    "    print(pressurevalue, df.columns[-1])\n",
    "    df['Sheet'] = pressurevalue\n",
    "    df.index.names = ['CurveID']\n",
    "    df.reset_index(inplace=True)\n",
    "    df['CurveID'] = df['CurveID'].astype(int)\n",
    "    df.set_index(['CurveID','Sheet'], inplace=True)\n",
    "    df = df.join(curves_df)\n",
    "    pressurevalues[pressurevalue] = df\n",
    "    \n",
    "\n",
    "tadm_data = pd.concat([pressurevalues[df] for df in pressurevalues],axis=0).set_index(['LiquidClassName',\n",
    "                                                                            'Volume',\n",
    "                                                                            'StepType',\n",
    "                                                                            'Channel',\n",
    "                                                                            'Time',\n",
    "                                                                            'StepNumber',\n",
    "                                                                            'TadmMode',\n",
    "                                                                            'TadmError'],append=True)#.to_csv('mergedsheet.csv')\n",
    "\n",
    "\n",
    "##Step 2 Get a Raw Data File and parse out \n",
    "class nmdx_file_parser:\n",
    "    \"\"\"\n",
    "    A class used to read raw data file(s) and convert to flat format.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    scrapeFile(file=None, env=None)\n",
    "        Scrapes data from one raw data file.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.file_data = {}\n",
    "\n",
    "    def readChannelData(file, sheet, channel):\n",
    "\n",
    "        channelData_all = pd.read_excel(io=file,sheet_name=sheet)\n",
    "        if len(channelData_all) > 0:\n",
    "            ChannelRawStart = channelData_all[channelData_all['Sample ID']=='Raw'].index.values[0] + 1\n",
    "            ChannelRawEnd = channelData_all[channelData_all['Sample ID']=='Normalized'].index.values[0] - 2\n",
    "            ChannelRaw = channelData_all.loc[ChannelRawStart:ChannelRawEnd]\n",
    "            ChannelRaw['Processing Step'] = 'Raw'\n",
    "\n",
    "            ChannelNormStart = channelData_all[channelData_all['Sample ID']=='Normalized'].index.values[0] + 1\n",
    "            ChannelNormEnd = channelData_all[channelData_all['Sample ID']=='SecondDerivative'].index.values[0] - 2\n",
    "            ChannelNorm = channelData_all.loc[ChannelNormStart:ChannelNormEnd]\n",
    "            ChannelNorm['Processing Step'] = 'Normalized'\n",
    "\n",
    "            Channel2ndStart = channelData_all[channelData_all['Sample ID']=='SecondDerivative'].index.values[0] + 1\n",
    "\n",
    "            if 'Modulated' in channelData_all['Sample ID'].unique():\n",
    "                Channel2ndEnd = channelData_all[channelData_all['Sample ID']=='Modulated'].index.values[0] - 2\n",
    "                ChannelModulatedStart = channelData_all[channelData_all['Sample ID']=='Modulated'].index.values[0] + 1\n",
    "                ChannelModulated = channelData_all.loc[ChannelModulatedStart:ChannelModulatedStart+len(ChannelRaw)]\n",
    "                ChannelModulated['Processing Step'] = 'Modulated'\n",
    "                Channel2nd = channelData_all.loc[Channel2ndStart:Channel2ndEnd]\n",
    "                Channel2nd['Processing Step'] = '2nd'\n",
    "\n",
    "                if len(ChannelRaw) == len(ChannelNorm) and len(ChannelRaw) == len(Channel2nd) and len(ChannelRaw) == len(ChannelModulated):\n",
    "\n",
    "                    ChannelFinal = pd.concat([ChannelRaw, ChannelNorm, Channel2nd, ChannelModulated],axis=0)\n",
    "                    ChannelFinal['Channel'] = channel\n",
    "                    ChannelFinal.set_index(['Test Guid', 'Replicate Number'],inplace=True)\n",
    "                else:\n",
    "                    print(\"Error in parsing Datablocks\")\n",
    "            else:\n",
    "                Channel2nd = channelData_all.loc[Channel2ndStart:Channel2ndStart+len(ChannelRaw)]\n",
    "                Channel2nd['Processing Step'] = '2nd'\n",
    "                #if len(ChannelRaw) == len(ChannelNorm) and len(ChannelRaw) == len(Channel2nd):\n",
    "                ChannelFinal = pd.concat([ChannelRaw, ChannelNorm, Channel2nd],axis=0)\n",
    "                ChannelFinal['Channel'] = channel\n",
    "                ChannelFinal.set_index(['Test Guid', 'Replicate Number'],inplace=True)\n",
    "\n",
    "\n",
    "        else:\n",
    "            ChannelFinal = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "        return ChannelFinal\n",
    "    \n",
    "    def readRawData(file):\n",
    "        channelDict = {'Green_470_510':'Green',\n",
    "                    'Yellow_530_555':'Yellow',\n",
    "                    'Orange_585_610':'Orange',\n",
    "                    'Red_625_660':'Red',\n",
    "                    'Far_Red_680_715':'Far_Red'}\n",
    "\n",
    "        Summary_Tab = pd.read_excel(io=file,sheet_name='Summary',header=2)\n",
    "        COC_Tab = pd.read_excel(io=file,sheet_name='Chain of Custody')\n",
    "        Summary_COC_Data = Summary_Tab.set_index(['Test Guid', 'Replicate Number']).join(COC_Tab.set_index(['Test Guid', 'Replicate Number']).loc[:, [x for x in COC_Tab.columns if x not in Summary_Tab.columns]])\n",
    "\n",
    "\n",
    "        channelDataDict = {}\n",
    "        for channel in channelDict:\n",
    "            channelDataDict[channel] = nmdx_file_parser.readChannelData(file, channel, channelDict[channel])\n",
    "        channelDataFinal = pd.concat([channelDataDict[df] for df in channelDataDict if len(channelDataDict[df])>0],axis=0)\n",
    "\n",
    "        \n",
    "        channelDataFinal.set_index(['Target Result Guid', 'Processing Step', 'Channel'],append=True,inplace=True)\n",
    "        for i in range(1,256):\n",
    "            if \"Readings \"+ str(i) not in channelDataFinal.columns:\n",
    "                channelDataFinal[\"Readings \"+str(i)] = np.nan\n",
    "        channelDataFinal_readings = channelDataFinal.loc[:, ['Readings '+str(i) for i in range(1,256)]]\n",
    "        channelDataFinal_summary = channelDataFinal.swaplevel(3,0).swaplevel(3,1).swaplevel(3,2)\n",
    "        channelDataFinal_summary = channelDataFinal_summary.loc['Raw'].drop(['Readings '+str(i) for i in range(1,256)],axis=1)\n",
    "\n",
    "        return Summary_COC_Data, channelDataFinal_summary, channelDataFinal_readings\n",
    "    \n",
    "    def retrieveConsumableLots(data, consumable_types=['Pcr Cartridge', 'Capture Plate', 'Test Strip NeuMoDx', 'Buffer', 'Release Reagent', 'Wash Reagent']):\n",
    "        \"\"\"\n",
    "        Retrieves Lot information for NMDX Consumables from Barcode String\n",
    "        :param consumable_types: list-like List of Consumables to get Data For.\n",
    "        \"\"\"\n",
    "    \n",
    "        for consumable_type in consumable_types:\n",
    "            data[consumable_type+\" Lot\"] = data[consumable_type+\" Barcode\"].str[18:24]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def retrieveConsumableSerials(data, consumable_types=['Pcr Cartridge', 'Capture Plate', 'Test Strip NeuMoDx', 'Buffer', 'Release Reagent', 'Wash Reagent']):\n",
    "        \"\"\"\n",
    "        Retrieves Consumable Serial information for NMDX Consumables from Barcode String\n",
    "        :param consumable_types: list-like List of Consumables to get Data For\n",
    "        \"\"\"\n",
    "        \n",
    "        for consumable_type in consumable_types:\n",
    "            data[consumable_type+\" Serial\"] = data[consumable_type+\" Barcode\"].str[27:32]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def retrieveConsumableExpiration(data, consumable_types=['Pcr Cartridge', 'Capture Plate', 'Test Strip NeuMoDx', 'Buffer', 'Release Reagent', 'Wash Reagent']):\n",
    "        \"\"\"\n",
    "        Retrieves Expiration Date information for NMDX Consumables from Barcode String\n",
    "        :param consumable_types: list-like List of Consumables to get Data For.\n",
    "        \"\"\"\n",
    "    \n",
    "        for consumable_type in consumable_types:\n",
    "            data[consumable_type+\" EXP Date\"] = data[consumable_type+\" Barcode\"].str[-6:].apply(lambda x: pd.to_datetime(arg=x, format=\"%y%m%d\"))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def scrapeFile(self, file, filename):\n",
    "           \n",
    "        #time = pd.Timestamp.now()\n",
    "\n",
    "        summary_coc, channel_summary, channel_readings = nmdx_file_parser.readRawData(file)\n",
    "        for col in channel_summary.columns:\n",
    "            if 'Barcode' in col:\n",
    "                channel_summary[col] = channel_summary[col].astype(str)\n",
    "                channel_summary[col] = channel_summary[col].str.replace(\"_x001D_\", \" \")\n",
    "        channel_summary = channel_summary.astype(object).where(pd.notna(channel_summary), None)\n",
    "\n",
    "\n",
    "        for col in summary_coc.columns:\n",
    "            if 'Barcode' in col:\n",
    "                summary_coc[col] = summary_coc[col].astype(str)\n",
    "                summary_coc[col] = summary_coc[col].str.replace(\"_x001D_\", \" \")\n",
    "            if 'ADP Position' in col:\n",
    "                summary_coc[col] = summary_coc[col].astype(str)\n",
    "        summary_coc = summary_coc.astype(object).where(pd.notna(summary_coc), None)\n",
    "        for col in summary_coc.loc[:, [col for col in summary_coc if 'Date' in col]].columns:\n",
    "            summary_coc[col] = pd.to_datetime(summary_coc[col], utc=False).apply(lambda x: x.replace(tzinfo=pytz.utc))\n",
    "        \n",
    "        channel_readings = channel_readings.astype(object).where(pd.notna(channel_readings), None)\n",
    "\n",
    "        channel_summary['File Source'] = filename\n",
    "        channel_readings['File Source'] = filename\n",
    "        summary_coc['File Source'] = filename\n",
    "        summary_coc.rename({'Flags':'Summary Flags'},axis=1,inplace=True)\n",
    "        channel_summary.rename({'Flags':'Channel Flags'},axis=1,inplace=True)\n",
    "        summary_coc = nmdx_file_parser.retrieveConsumableLots(summary_coc)\n",
    "        summary_coc = nmdx_file_parser.retrieveConsumableSerials(summary_coc)\n",
    "        summary_coc = nmdx_file_parser.retrieveConsumableExpiration(summary_coc)\n",
    "        \n",
    "        \n",
    "\n",
    "        flat_data = summary_coc.join(channel_summary.loc[:, [x for x in channel_summary.columns if x not in summary_coc.columns]]).join(channel_readings.loc[:, [x for x in channel_readings.columns if x not in channel_summary.columns]])\n",
    "        \n",
    "        ##Add Target Result / Localized Result columns if not in flat_data columns\n",
    "        if 'Localized Result' not in flat_data.columns:\n",
    "            flat_data['Localized Result'] = np.nan\n",
    "        \n",
    "        if 'Target Result' not in flat_data.columns:\n",
    "            flat_data['Target Result'] = np.nan\n",
    "\n",
    "        return flat_data.reset_index()\n",
    "myParser = nmdx_file_parser()\n",
    "raw_data = myParser.scrapeFile(file='Example/RawDataExport.NDX-03.12000112.2207211558.016653D0.xlsx', filename='test')\n",
    "raw_data.drop_duplicates(subset=['Test Guid'], inplace=True)\n",
    "raw_data = raw_data[['Test Guid', 'Sample ID', 'Start Date/Time', 'LHPA Start Date Time', 'LHPB Start Date Time', 'LHPC Start Date Time', 'PCR Start Date Time', 'LHPA ADP Position', 'LHPB ADP Position', 'LHPC ADP Position']]\n",
    "\n",
    "def closest_match(item, tadm_reference, channel, main_process, processGroupsTimes):\n",
    "\n",
    "    ##This is the function that does the searching\n",
    "    def find_tadms(channel, repeat_offset=0):\n",
    "        ##Get Test Guid of Sample\n",
    "        test_guid = item['Test Guid'].values[0]\n",
    "        \n",
    "        ##Convert Associated Start Date Time to be utc agnostic\n",
    "        item[main_process+' Start Date Time'] = item[main_process+' Start Date Time'].apply(lambda x: x.replace(tzinfo=pytz.utc))\n",
    "        \n",
    "        ##Get Time of associated Sample\n",
    "        time = item[main_process+' Start Date Time'].astype('datetime64[ns]').values[0]\n",
    "        \n",
    "        ##Determine the processing group sample is associated with\n",
    "        processGroupTimes = processGroupsTimes[main_process]\n",
    "        processGroupTimes[main_process+\" Start Date Time\"] = processGroupTimes[main_process+\" Start Date Time\"].astype('datetime64[ns]')\n",
    "        processGroupTimes['Reference Time'] = time\n",
    "        processGroupTimes['Delta Time'] = abs(processGroupTimes[main_process+\" Start Date Time\"]-processGroupTimes['Reference Time'])\n",
    "        \n",
    "        ##Determine Minimum and Maximum Bounds for time allowed to search within.\n",
    "        minimum_time_bound_index = processGroupTimes.loc[processGroupTimes['Delta Time']==processGroupTimes['Delta Time'].min(), main_process+\" Start Date Time\"].index.values[0]\n",
    "        \n",
    "        ##Apply a -1 run group offset in the case of a first time repeated sample.\n",
    "        minimum_time_bound_index = minimum_time_bound_index - repeat_offset\n",
    "\n",
    "        ##Get Value of minimum_time_bound\n",
    "        minimum_time_bound = processGroupTimes.loc[minimum_time_bound_index, main_process+\" Start Date Time\"] \n",
    "\n",
    "        if minimum_time_bound_index+1 < len(processGroupTimes):\n",
    "            maximum_time_bound = processGroupTimes.loc[minimum_time_bound_index+1, main_process+\" Start Date Time\"] + np.timedelta64(30, 's')\n",
    "        else:\n",
    "            maximum_time_bound = minimum_time_bound + np.timedelta64(5, 'm')\n",
    "        \n",
    "        #print(minimum_time_bound)\n",
    "        #print(maximum_time_bound)\n",
    "\n",
    "        \n",
    "        ##Filter TADM Reference to only be for the Channel and Time Range allowed to search within\n",
    "        tadm_reference_channel = tadm_reference.reset_index(['Channel', 'LiquidClassName', 'StepType','Time'])\n",
    "        tadm_reference_channel['Time'] = tadm_reference_channel['Time'].astype('datetime64[ns]')\n",
    "        tadm_reference_channel = tadm_reference_channel[((tadm_reference_channel['Channel']==channel)&\n",
    "                                                        (tadm_reference_channel['Time']>minimum_time_bound)&\n",
    "                                                        (tadm_reference_channel['Time']<maximum_time_bound))]\n",
    "        \n",
    "        \n",
    "        ##Determine Delta Time from Time observed for sample process\n",
    "        tadm_reference_channel['Reference Time'] = time\n",
    "        tadm_reference_channel['Delta Time'] = (tadm_reference_channel['Time']  - tadm_reference_channel['Reference Time']).astype('timedelta64[s]')\n",
    "        #tadm_reference_channel = tadm_reference_channel.loc[((tadm_reference_channel['Delta Time']<offset_max)&(tadm_reference_channel['Delta Time']>offset_min))]\n",
    "\n",
    "        ##Add Test Guid to TADM reference\n",
    "        tadm_reference_channel['Test Guid'] = test_guid\n",
    "        tadm_reference_channel = tadm_reference_channel.reset_index()[['CurveID', 'Test Guid','Time','LiquidClassName','Sheet','Delta Time', 'StepType', 'Volume']].sort_values('Delta Time')\n",
    "        \n",
    "        ##Filter to make sure that we are only grabbing TADMs that we would expect based on process.\n",
    "        if main_process == 'LHPB':\n",
    "            tadm_reference_channel = tadm_reference_channel[((tadm_reference_channel['LiquidClassName'].str.contains(main_process))|(tadm_reference_channel['LiquidClassName'].str.contains('High')))]\n",
    "        else:\n",
    "            tadm_reference_channel = tadm_reference_channel[((tadm_reference_channel['LiquidClassName'].str.contains(main_process)))]\n",
    "\n",
    "        ##Drop any duplicates that may have been found, keep lowest time delta.\n",
    "        tadm_reference_channel.drop_duplicates(['LiquidClassName','StepType'],keep='first',inplace=True)\n",
    "        \n",
    "        \n",
    "        return tadm_reference_channel\n",
    "\n",
    "    ##Determine which Channel to work with and if a sample is a aborted, or repeated sample.\n",
    "    if pd.isnull(channel) or \"nan\" in channel:\n",
    "        print(\"channel not found error\")\n",
    "        return\n",
    "\n",
    "    elif \",\" in channel:\n",
    "        channel_1 = pd.to_numeric(channel[-1])\n",
    "        set1 = find_tadms(channel_1)\n",
    "        channel_2 = pd.to_numeric(channel[0])\n",
    "        set2 = find_tadms(channel_2, repeat_offset=1)\n",
    "        return pd.concat([set1, set2],axis=0).drop_duplicates(['CurveID'],keep='first')\n",
    "    else:\n",
    "        channel = pd.to_numeric(channel)\n",
    "        set1 = find_tadms(channel)\n",
    "        return set1\n",
    "\n",
    "processGroups = {}\n",
    "for process in ['LHPA', 'LHPB', 'LHPC']:\n",
    "    processGroups[process] = raw_data[[process+' Start Date Time']].drop_duplicates([process+' Start Date Time']).dropna().sort_values(process+' Start Date Time').reset_index(drop=True)\n",
    "conversion_frame = pd.DataFrame(columns=['CurveID', 'Test Guid','Time','LiquidClassName', 'StepType', 'Delta Time', 'Volume'])\n",
    "for id in raw_data.index.values:\n",
    "    for process in ['LHPA', 'LHPB', 'LHPC']:\n",
    "        conversion_frame = pd.concat([conversion_frame, closest_match(raw_data.loc[[id]], tadm_data, raw_data.loc[id, process+\" ADP Position\"], process, processGroups)],axis=0)\n",
    "    print(len(conversion_frame.sort_values('Time')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CurveID</th>\n",
       "      <th>Time</th>\n",
       "      <th>LiquidClassName</th>\n",
       "      <th>StepType</th>\n",
       "      <th>Delta Time</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Sheet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Guid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1626</td>\n",
       "      <td>2022-07-21 11:34:47</td>\n",
       "      <td>NMDX_LHPA_Buffer_TADM</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>25.0</td>\n",
       "      <td>700</td>\n",
       "      <td>PressureValues0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1634</td>\n",
       "      <td>2022-07-21 11:35:04</td>\n",
       "      <td>NMDX_LHPA_CapPlate_Urine_TADM</td>\n",
       "      <td>Dispensing</td>\n",
       "      <td>42.0</td>\n",
       "      <td>700</td>\n",
       "      <td>PressureValues4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1642</td>\n",
       "      <td>2022-07-21 11:35:32</td>\n",
       "      <td>NMDX_LHPA_Urine_TADM</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PressureValues7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1789</td>\n",
       "      <td>2022-07-21 11:53:44</td>\n",
       "      <td>NMDX_LHPB_CapPlate_Urine_TADM</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>57.0</td>\n",
       "      <td>946</td>\n",
       "      <td>PressureValues10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1796</td>\n",
       "      <td>2022-07-21 11:53:50</td>\n",
       "      <td>NeuMoDx_HighVolumeFilter_Air_DispenseSurface</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>63.0</td>\n",
       "      <td>10</td>\n",
       "      <td>PressureValues23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1803</td>\n",
       "      <td>2022-07-21 11:55:17</td>\n",
       "      <td>NMDX_LHPB_Urine_30uL_TADM</td>\n",
       "      <td>Dispensing</td>\n",
       "      <td>150.0</td>\n",
       "      <td>946</td>\n",
       "      <td>PressureValues12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1905</td>\n",
       "      <td>2022-07-21 12:18:30</td>\n",
       "      <td>NeuMoDx_LHPC1_StandardVolumeFilter_Air_Dispens...</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>-586.0</td>\n",
       "      <td>10</td>\n",
       "      <td>PressureValues24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1920</td>\n",
       "      <td>2022-07-21 12:20:29</td>\n",
       "      <td>NMDX_LHPC1_Urine_TADM</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>-467.0</td>\n",
       "      <td>75</td>\n",
       "      <td>PressureValues14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1921</td>\n",
       "      <td>2022-07-21 12:20:54</td>\n",
       "      <td>NMDX_LHPC2_urine_TADM</td>\n",
       "      <td>Dispensing</td>\n",
       "      <td>-442.0</td>\n",
       "      <td>80</td>\n",
       "      <td>PressureValues22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1924</td>\n",
       "      <td>2022-07-21 12:21:38</td>\n",
       "      <td>NMDX_LHPC2_urine_TADM</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>-398.0</td>\n",
       "      <td>18</td>\n",
       "      <td>PressureValues20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1953</td>\n",
       "      <td>2022-07-21 12:25:03</td>\n",
       "      <td>NeuMoDx_LHPC2_StandardVolumeFilter_Air_Dispens...</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>-193.0</td>\n",
       "      <td>10</td>\n",
       "      <td>PressureValues25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1964</td>\n",
       "      <td>2022-07-21 12:29:16</td>\n",
       "      <td>NMDX_LHPC2_urine_TADM</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17</td>\n",
       "      <td>PressureValues19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1965</td>\n",
       "      <td>2022-07-21 12:29:27</td>\n",
       "      <td>NeuMoDx_LHPC2_StandardVolumeFilter_Air_Dispens...</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>71.0</td>\n",
       "      <td>10</td>\n",
       "      <td>PressureValues25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e42e1f8-1209-ed11-85f4-5cf370a20867</th>\n",
       "      <td>1966</td>\n",
       "      <td>2022-07-21 12:33:33</td>\n",
       "      <td>NeuMoDx_LHPC1_StandardVolumeFilter_Air_Dispens...</td>\n",
       "      <td>Aspirating</td>\n",
       "      <td>317.0</td>\n",
       "      <td>10</td>\n",
       "      <td>PressureValues24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     CurveID                Time  \\\n",
       "Test Guid                                                          \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1626 2022-07-21 11:34:47   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1634 2022-07-21 11:35:04   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1642 2022-07-21 11:35:32   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1789 2022-07-21 11:53:44   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1796 2022-07-21 11:53:50   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1803 2022-07-21 11:55:17   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1905 2022-07-21 12:18:30   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1920 2022-07-21 12:20:29   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1921 2022-07-21 12:20:54   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1924 2022-07-21 12:21:38   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1953 2022-07-21 12:25:03   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1964 2022-07-21 12:29:16   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1965 2022-07-21 12:29:27   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867    1966 2022-07-21 12:33:33   \n",
       "\n",
       "                                                                        LiquidClassName  \\\n",
       "Test Guid                                                                                 \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867                              NMDX_LHPA_Buffer_TADM   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867                      NMDX_LHPA_CapPlate_Urine_TADM   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867                               NMDX_LHPA_Urine_TADM   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867                      NMDX_LHPB_CapPlate_Urine_TADM   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867       NeuMoDx_HighVolumeFilter_Air_DispenseSurface   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867                          NMDX_LHPB_Urine_30uL_TADM   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  NeuMoDx_LHPC1_StandardVolumeFilter_Air_Dispens...   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867                              NMDX_LHPC1_Urine_TADM   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867                              NMDX_LHPC2_urine_TADM   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867                              NMDX_LHPC2_urine_TADM   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  NeuMoDx_LHPC2_StandardVolumeFilter_Air_Dispens...   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867                              NMDX_LHPC2_urine_TADM   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  NeuMoDx_LHPC2_StandardVolumeFilter_Air_Dispens...   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  NeuMoDx_LHPC1_StandardVolumeFilter_Air_Dispens...   \n",
       "\n",
       "                                        StepType  Delta Time Volume  \\\n",
       "Test Guid                                                             \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating        25.0    700   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Dispensing        42.0    700   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating        70.0      0   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating        57.0    946   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating        63.0     10   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Dispensing       150.0    946   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating      -586.0     10   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating      -467.0     75   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Dispensing      -442.0     80   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating      -398.0     18   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating      -193.0     10   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating        60.0     17   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating        71.0     10   \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  Aspirating       317.0     10   \n",
       "\n",
       "                                                 Sheet  \n",
       "Test Guid                                               \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867   PressureValues0  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867   PressureValues4  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867   PressureValues7  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues10  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues23  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues12  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues24  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues14  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues22  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues20  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues25  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues19  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues25  \n",
       "7e42e1f8-1209-ed11-85f4-5cf370a20867  PressureValues24  "
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversion_frame.set_index('Test Guid').loc['7e42e1f8-1209-ed11-85f4-5cf370a20867'].sort_values('Time')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ca8ae99df12afeecc637c741eef9d8fc79f3c8a2a662f29db32bfeb3c8d3abb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
