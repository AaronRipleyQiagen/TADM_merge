{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Raw Data from file: P:/USERS/PSG/Barcelona TADMs/TADMs/RawDataExport.Qiagen Barcelona.23000100.2212071659.076F8FAE.xlsx\n",
      "reading data from folder: P:/USERS/PSG/Barcelona TADMs/TADMs/13f3a9e30ea34ab08681d3361ededf31\n",
      "Number of TADM entries Loaded: 1619\n",
      "reading data from folder: P:/USERS/PSG/Barcelona TADMs/TADMs/4ad27279949c441f9298df8453c50daf\n",
      "Number of TADM entries Loaded: 1715\n",
      "reading data from folder: P:/USERS/PSG/Barcelona TADMs/TADMs/6fde45db1a274ab38f2713e45bf0e1d5\n",
      "Number of TADM entries Loaded: 1841\n",
      "reading data from folder: P:/USERS/PSG/Barcelona TADMs/TADMs/7e313b64c13c4655a8d7d2681d357e02\n",
      "Number of TADM entries Loaded: 3726\n",
      "reading data from folder: P:/USERS/PSG/Barcelona TADMs/TADMs/dc512d26b1b24f7c9667849c53176519\n",
      "Number of TADM entries Loaded: 3933\n",
      "reading data from folder: P:/USERS/PSG/Barcelona TADMs/TADMs/e84a95e3ab0145338cf18aa1522a6fab\n",
      "Number of TADM entries Loaded: 4189\n",
      "PY_VAR18 PY_VAR19 PY_VAR20 PY_VAR21 PY_VAR22 PY_VAR23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 677\u001b[0m\n\u001b[0;32m    675\u001b[0m root\u001b[39m.\u001b[39mgeometry(windowsize)\n\u001b[0;32m    676\u001b[0m my_gui \u001b[39m=\u001b[39m UI(root)\n\u001b[1;32m--> 677\u001b[0m root\u001b[39m.\u001b[39;49mmainloop()\n",
      "File \u001b[1;32mc:\\Users\\RipleyA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py:1458\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmainloop\u001b[39m(\u001b[39mself\u001b[39m, n\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m   1457\u001b[0m     \u001b[39m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1458\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtk\u001b[39m.\u001b[39;49mmainloop(n)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pytz\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter.filedialog import askopenfilenames, asksaveasfilename, askdirectory\n",
    "import uuid\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class UI(Frame):    \n",
    "    def __init__(self, master=None):\n",
    "        \n",
    "        Frame.__init__(self, master)\n",
    "        \n",
    "        self.myParser = nmdx_file_parser()\n",
    "        \n",
    "        self.raw_data = pd.DataFrame()\n",
    "        self.tadm_data = pd.DataFrame()\n",
    "\n",
    "        self.bakFrame = tk.Frame(master, bg='white')\n",
    "        self.bakFrame.place(relx=0, rely=0, relheight=1, relwidth=1, anchor='nw')\n",
    "\n",
    "        self.GetDataButton = tk.Button(self.bakFrame, text=\"Select Raw Data Files\", bg='white', command=self.load_raw_data)\n",
    "        self.GetDataButton.place(relx=0.04, rely=0.05, anchor='nw', relwidth=0.2, relheight=0.25)\n",
    "\n",
    "        self.GetTADMReferenceButton = tk.Button(self.bakFrame, text=\"Select TADM Reference Directory\", bg='white', command=self.get_tadm_data)\n",
    "        self.GetTADMReferenceButton.place(relx=0.28, rely=0.05, anchor='nw', relwidth=0.2, relheight=0.25)\n",
    "\n",
    "        self.ProcessDataButton = tk.Button(self.bakFrame, text=\"Process Data\", bg='white', command=self.process_data)\n",
    "        self.ProcessDataButton.place(relx=0.52, rely=0.05, anchor='nw', relwidth=0.2, relheight=0.25)\n",
    "        \n",
    "        self.SaveDataButton = tk.Button(self.bakFrame, text=\"Save Data\", bg='white', command=self.save_data)\n",
    "        self.SaveDataButton.place(relx=0.76, rely=0.05, anchor='nw', relwidth=0.2, relheight=0.25)\n",
    "\n",
    "        self.resetButton = tk.Button(self.bakFrame, text=\"Clear Data\", bg='white', command=self.clearData)\n",
    "        self.resetButton.place(relx=0.28, rely=0.35, anchor='nw', relwidth=0.48, relheight=0.20)\n",
    "\n",
    "        ##Row 1 of Options\n",
    "        self.includeXPCR = IntVar()\n",
    "        self.includeXPCR.set(0)\n",
    "        self.includeXPCRButton = tk.Checkbutton(self.bakFrame, text=\"Include XPCR Module Info\", onvalue=1, offvalue=0, variable=self.includeXPCR, bg='white')\n",
    "        self.includeXPCRButton.place(relx=0.025, rely=0.575, anchor='nw', relwidth=0.30, relheight=0.1)\n",
    "\n",
    "        self.includeHM = IntVar()\n",
    "        self.includeHM.set(0)\n",
    "        self.includeHMButton = tk.Checkbutton(self.bakFrame, text=\"Include Heater Module Info\", onvalue=1, offvalue=0, variable=self.includeHM, bg='white')\n",
    "        self.includeHMButton.place(relx=0.35, rely=0.575, anchor='nw', relwidth=0.30, relheight=0.1)\n",
    "\n",
    "        self.includeTS = IntVar()\n",
    "        self.includeTS.set(0)\n",
    "        self.includeTSButton = tk.Checkbutton(self.bakFrame, text=\"Include Test Strip Position Info\", onvalue=1, offvalue=0, variable=self.includeTS, bg='white')\n",
    "        self.includeTSButton.place(relx=0.675, rely=0.575, anchor='nw', relwidth=0.30, relheight=0.1)\n",
    "\n",
    "        ##Row 2 of Options\n",
    "        self.includeSP = IntVar()\n",
    "        self.includeSP.set(0)\n",
    "        self.includeSPButton = tk.Checkbutton(self.bakFrame, text=\"Include Sample Processing Info\", onvalue=1, offvalue=0, variable=self.includeSP, bg='white')\n",
    "        self.includeSPButton.place(relx=0.025, rely=0.70, anchor='nw', relwidth=0.30, relheight=0.1)\n",
    "\n",
    "        self.includeCL = IntVar()\n",
    "        self.includeCL.set(0)\n",
    "        self.includeConsumableLotButton = tk.Checkbutton(self.bakFrame, text=\"Include Consumable Lot Info\", onvalue=1, offvalue=0, variable=self.includeCL, bg='white')\n",
    "        self.includeConsumableLotButton.place(relx=0.35, rely=0.70, anchor='nw', relwidth=0.30, relheight=0.1)\n",
    "\n",
    "        self.includeCR = IntVar()\n",
    "        self.includeCR.set(0)\n",
    "        self.includeCRButton = tk.Checkbutton(self.bakFrame, text=\"Include Channel Result Info\", onvalue=1, offvalue=0, variable=self.includeCR, bg='white')\n",
    "        self.includeCRButton.place(relx=0.675, rely=0.70, anchor='nw', relwidth=0.30, relheight=0.1)\n",
    "    def load_raw_data(self):\n",
    "        self.ReadingLabels = tk.Label(self.bakFrame, text=\"Parsing Raw Data\", bg='blue', fg='white')\n",
    "        self.ReadingLabels.place(relx=0, rely=0.825, anchor='nw', relwidth=1, relheight=0.20)\n",
    "        \n",
    "        files = [('XLSX', '*.xlsx')] \n",
    "\n",
    "        files = askopenfilenames(filetypes = files, defaultextension = files)\n",
    "        for file in files:\n",
    "            print(\"Reading Raw Data from file: \"+str(file))\n",
    "            self.raw_data = pd.concat([self.raw_data,self.myParser.scrapeFile(file=file, filename='test')])\n",
    "            self.myParser.getADFParameters(file)\n",
    "            \n",
    "        self.myTadmHelper = TadmHelper(self.raw_data)\n",
    "        self.ReadingLabels.destroy()  \n",
    "    def get_tadm_data(self):\n",
    "        self.ReadingLabels = tk.Label(self.bakFrame, text=\"Parsing TADM Data\", bg='blue', fg='white')\n",
    "        self.ReadingLabels.place(relx=0, rely=0.825, anchor='nw', relwidth=1, relheight=0.20)\n",
    "        self.bakFrame.update()\n",
    "        file_dir = askdirectory()\n",
    "        for folder in os.listdir(file_dir):\n",
    "            \n",
    "            if os.path.isdir(file_dir+\"/\"+folder):\n",
    "                self.myTadmHelper.get_tadms(file_dir+\"/\"+folder)\n",
    "\n",
    "                print(\"reading data from folder: \"+file_dir+\"/\"+folder)\n",
    "            \n",
    "                print(\"Number of TADM entries Loaded: \"+str(len(self.myTadmHelper.tadm_data)))\n",
    "        self.ReadingLabels.destroy()\n",
    "    def process_data(self):\n",
    "        self.ReadingLabels = tk.Label(self.bakFrame, text=\"Matching TADM data with NMDX Data\", bg='blue', fg='white')\n",
    "        self.ReadingLabels.place(relx=0, rely=0.825, anchor='nw', relwidth=1, relheight=0.20)\n",
    "        self.bakFrame.update()\n",
    "        #try:\n",
    "        self.myTadmHelper.tadm_hunter()\n",
    "        #except:\n",
    "        #print(\"Failed to process data.\")\n",
    "        self.ReadingLabels.destroy()\n",
    "    def save_data(self):\n",
    "        print(self.includeXPCR, self.includeHM, self.includeTS, self.includeSP, self.includeCL, self.includeCR)\n",
    "        self.ReadingLabels = tk.Label(self.bakFrame, text=\"Exporting CSV File of Matched TADM Data\", bg='blue', fg='white')\n",
    "        self.ReadingLabels.place(relx=0, rely=0.825, anchor='nw', relwidth=1, relheight=0.15)\n",
    "        self.bakFrame.update()\n",
    "        try:\n",
    "            tadm_output = self.myTadmHelper.tadm_merger(include_XPCR_info=self.includeXPCR.get(),include_HM_info=self.includeHM.get(),include_TS_info=self.includeTS.get(),include_SP_info=self.includeSP.get(),include_ConsLot_info=self.includeCL.get(), include_ChannelResult_info=self.includeCR.get())\n",
    "            output_dir = asksaveasfilename(title=\"Choose where to save TADM Data\", defaultextension=\".xlsx\", initialfile=\"TADM_output\", filetypes=[(\"CSV\", \"*.csv\")])\n",
    "            tadm_output.to_csv(output_dir)\n",
    "        except:\n",
    "            print(\"Failed to save Data.\")\n",
    "        self.ReadingLabels.destroy()\n",
    "    def clearData(self):\n",
    "        self.ReadingLabels = tk.Label(self.bakFrame, text=\"Clearing Data\", bg='blue', fg='white')\n",
    "        self.ReadingLabels.place(relx=0, rely=0.825, anchor='nw', relwidth=1, relheight=0.20)\n",
    "        self.bakFrame.update()\n",
    "        try:\n",
    "            self.raw_data = pd.DataFrame()\n",
    "            del self.myTadmHelper\n",
    "        except:\n",
    "            print(\"Failed to clear Data from App.\")\n",
    "        self.ReadingLabels.destroy()\n",
    "\n",
    "class nmdx_file_parser:\n",
    "    \"\"\"\n",
    "    A class used to read raw data file(s) and convert to flat format.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    scrapeFile(file=None, env=None)\n",
    "        Scrapes data from one raw data file.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.file_data = {}\n",
    "        self.adf_TADM_order = {}\n",
    "    \n",
    "    def readChannelData(file, sheet, channel):\n",
    "\n",
    "        channelData_all = pd.read_excel(io=file,sheet_name=sheet)\n",
    "        if len(channelData_all) > 0:\n",
    "            ChannelRawStart = channelData_all[channelData_all['Sample ID']=='Raw'].index.values[0] + 1\n",
    "            ChannelRawEnd = channelData_all[channelData_all['Sample ID']=='Normalized'].index.values[0] - 2\n",
    "            ChannelRaw = channelData_all.loc[ChannelRawStart:ChannelRawEnd]\n",
    "            ChannelRaw['Processing Step'] = 'Raw'\n",
    "\n",
    "            ChannelNormStart = channelData_all[channelData_all['Sample ID']=='Normalized'].index.values[0] + 1\n",
    "            ChannelNormEnd = channelData_all[channelData_all['Sample ID']=='SecondDerivative'].index.values[0] - 2\n",
    "            ChannelNorm = channelData_all.loc[ChannelNormStart:ChannelNormEnd]\n",
    "            ChannelNorm['Processing Step'] = 'Normalized'\n",
    "\n",
    "            Channel2ndStart = channelData_all[channelData_all['Sample ID']=='SecondDerivative'].index.values[0] + 1\n",
    "\n",
    "            if 'Modulated' in channelData_all['Sample ID'].unique():\n",
    "                Channel2ndEnd = channelData_all[channelData_all['Sample ID']=='Modulated'].index.values[0] - 2\n",
    "                ChannelModulatedStart = channelData_all[channelData_all['Sample ID']=='Modulated'].index.values[0] + 1\n",
    "                ChannelModulated = channelData_all.loc[ChannelModulatedStart:ChannelModulatedStart+len(ChannelRaw)]\n",
    "                ChannelModulated['Processing Step'] = 'Modulated'\n",
    "                Channel2nd = channelData_all.loc[Channel2ndStart:Channel2ndEnd]\n",
    "                Channel2nd['Processing Step'] = '2nd'\n",
    "\n",
    "                if len(ChannelRaw) == len(ChannelNorm) and len(ChannelRaw) == len(Channel2nd) and len(ChannelRaw) == len(ChannelModulated):\n",
    "\n",
    "                    ChannelFinal = pd.concat([ChannelRaw, ChannelNorm, Channel2nd, ChannelModulated],axis=0)\n",
    "                    ChannelFinal['Channel'] = channel\n",
    "                    ChannelFinal.set_index(['Test Guid', 'Replicate Number'],inplace=True)\n",
    "                else:\n",
    "                    print(\"Error in parsing Datablocks\")\n",
    "            else:\n",
    "                Channel2nd = channelData_all.loc[Channel2ndStart:Channel2ndStart+len(ChannelRaw)]\n",
    "                Channel2nd['Processing Step'] = '2nd'\n",
    "                #if len(ChannelRaw) == len(ChannelNorm) and len(ChannelRaw) == len(Channel2nd):\n",
    "                ChannelFinal = pd.concat([ChannelRaw, ChannelNorm, Channel2nd],axis=0)\n",
    "                ChannelFinal['Channel'] = channel\n",
    "                ChannelFinal.set_index(['Test Guid', 'Replicate Number'],inplace=True)\n",
    "\n",
    "\n",
    "        else:\n",
    "            ChannelFinal = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "        return ChannelFinal\n",
    "    \n",
    "    def readRawData(file):\n",
    "        channelDict = {'Green_470_510':'Green',\n",
    "                    'Yellow_530_555':'Yellow',\n",
    "                    'Orange_585_610':'Orange',\n",
    "                    'Red_625_660':'Red',\n",
    "                    'Far_Red_680_715':'Far_Red'}\n",
    "\n",
    "        Summary_Tab = pd.read_excel(io=file,sheet_name='Summary',header=2)\n",
    "        COC_Tab = pd.read_excel(io=file,sheet_name='Chain of Custody')\n",
    "        Summary_COC_Data = Summary_Tab.set_index(['Test Guid', 'Replicate Number']).join(COC_Tab.set_index(['Test Guid', 'Replicate Number']).loc[:, [x for x in COC_Tab.columns if x not in Summary_Tab.columns]])\n",
    "\n",
    "\n",
    "        channelDataDict = {}\n",
    "        for channel in channelDict:\n",
    "            channelDataDict[channel] = nmdx_file_parser.readChannelData(file, channel, channelDict[channel])\n",
    "        channelDataFinal = pd.concat([channelDataDict[df] for df in channelDataDict if len(channelDataDict[df])>0],axis=0)\n",
    "\n",
    "        \n",
    "        channelDataFinal.set_index(['Target Result Guid', 'Processing Step', 'Channel'],append=True,inplace=True)\n",
    "        for i in range(1,256):\n",
    "            if \"Readings \"+ str(i) not in channelDataFinal.columns:\n",
    "                channelDataFinal[\"Readings \"+str(i)] = np.nan\n",
    "        channelDataFinal_readings = channelDataFinal.loc[:, ['Readings '+str(i) for i in range(1,256)]]\n",
    "        channelDataFinal_summary = channelDataFinal.swaplevel(3,0).swaplevel(3,1).swaplevel(3,2)\n",
    "        channelDataFinal_summary = channelDataFinal_summary.loc['Raw'].drop(['Readings '+str(i) for i in range(1,256)],axis=1)\n",
    "\n",
    "        return Summary_COC_Data, channelDataFinal_summary, channelDataFinal_readings\n",
    "    \n",
    "    def retrieveConsumableLots(data, consumable_types=['Pcr Cartridge', 'Capture Plate', 'Test Strip NeuMoDx', 'Buffer', 'Release Reagent', 'Wash Reagent']):\n",
    "        \"\"\"\n",
    "        Retrieves Lot information for NMDX Consumables from Barcode String\n",
    "        :param consumable_types: list-like List of Consumables to get Data For.\n",
    "        \"\"\"\n",
    "    \n",
    "        for consumable_type in consumable_types:\n",
    "            data[consumable_type+\" Lot\"] = data[consumable_type+\" Barcode\"].str[18:24]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def retrieveConsumableSerials(data, consumable_types=['Pcr Cartridge', 'Capture Plate', 'Test Strip NeuMoDx', 'Buffer', 'Release Reagent', 'Wash Reagent']):\n",
    "        \"\"\"\n",
    "        Retrieves Consumable Serial information for NMDX Consumables from Barcode String\n",
    "        :param consumable_types: list-like List of Consumables to get Data For\n",
    "        \"\"\"\n",
    "        \n",
    "        for consumable_type in consumable_types:\n",
    "            data[consumable_type+\" Serial\"] = data[consumable_type+\" Barcode\"].str[27:32]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def retrieveConsumableExpiration(data, consumable_types=['Pcr Cartridge', 'Capture Plate', 'Test Strip NeuMoDx', 'Buffer', 'Release Reagent', 'Wash Reagent']):\n",
    "        \"\"\"\n",
    "        Retrieves Expiration Date information for NMDX Consumables from Barcode String\n",
    "        :param consumable_types: list-like List of Consumables to get Data For.\n",
    "        \"\"\"\n",
    "    \n",
    "        for consumable_type in consumable_types:\n",
    "            data[consumable_type+\" EXP Date\"] = data[consumable_type+\" Barcode\"].str[-6:].apply(lambda x: pd.to_datetime(arg=x, format=\"%y%m%d\"))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def getRawMinusBlankCheckReads(self, data):\n",
    "        \"\"\"\n",
    "        A Function used to calculate the Difference between the First three Raw Readings and Blank Check Values for each target result included in dataset provided\n",
    "        Parameters\n",
    "        ----------\n",
    "        data (pandas.DataFrame) = DataFrame to be used for Calculation.\n",
    "        \"\"\"\n",
    "        RawReadsMinusBlankCheckFrame = data.reset_index()[['Processing Step', 'Test Guid', 'Replicate Number', 'Target Result Guid']+['Readings 1', 'Readings 2', 'Readings 3', 'Blank Reading']].copy()\n",
    "        RawReadsMinusBlankCheckFrame.set_index(['Processing Step', 'Test Guid', 'Replicate Number', 'Target Result Guid'],inplace=True)\n",
    "        RawReadsMinusBlankCheckFrame_Raw = RawReadsMinusBlankCheckFrame.loc['Raw']\n",
    "        RawReadsMinusBlankCheckFrame_Raw['Blank Check - 1st 3 Reads'] = RawReadsMinusBlankCheckFrame_Raw[['Readings 1', 'Readings 2', 'Readings 3']].mean(axis=1) - RawReadsMinusBlankCheckFrame_Raw['Blank Reading']\n",
    "        RawReadsMinusBlankCheckFrame = RawReadsMinusBlankCheckFrame.join(RawReadsMinusBlankCheckFrame_Raw[['Blank Check - 1st 3 Reads']])\n",
    "        data['Blank Check - 1st 3 Reads'] = RawReadsMinusBlankCheckFrame['Blank Check - 1st 3 Reads'].values\n",
    "    \n",
    "    def channelParametersFlattener(self, data, stats=['Target Name', 'Localized Result', 'Ct', 'End Point Fluorescence', 'EPR', 'Max Peak Height', 'Baseline Slope', 'Baseline Y Intercept', 'Blank Check - 1st 3 Reads']):\n",
    "        \"\"\"\n",
    "        Retrieves Channel Specific stats and returns them all channels in one-dimmensional column.\n",
    "        stats:  Which Stats to flatten.\n",
    "        \"\"\"\n",
    "        channel_stats = data.reset_index().drop_duplicates(['Test Guid', 'Channel', 'Replicate Number']).set_index(['Test Guid', 'Replicate Number']).loc[:, stats+['Channel']]\n",
    "\n",
    "        channel_stats = channel_stats.reset_index().pivot(columns='Channel',values=stats,index=['Test Guid', 'Replicate Number'])\n",
    "        channel_stats.columns = [y+\" \"+x for (x,y) in channel_stats.columns]\n",
    "        data = data.reset_index().set_index(['Test Guid', 'Replicate Number']).join(channel_stats)\n",
    "        return data\n",
    "\n",
    "    def scrapeFile(self, file, filename):\n",
    "           \n",
    "        #time = pd.Timestamp.now()\n",
    "\n",
    "        summary_coc, channel_summary, channel_readings = nmdx_file_parser.readRawData(file)\n",
    "        for col in channel_summary.columns:\n",
    "            if 'Barcode' in col:\n",
    "                channel_summary[col] = channel_summary[col].astype(str)\n",
    "                channel_summary[col] = channel_summary[col].str.replace(\"_x001D_\", \" \")\n",
    "        channel_summary = channel_summary.astype(object).where(pd.notna(channel_summary), None)\n",
    "\n",
    "\n",
    "        for col in summary_coc.columns:\n",
    "            if 'Barcode' in col:\n",
    "                summary_coc[col] = summary_coc[col].astype(str)\n",
    "                summary_coc[col] = summary_coc[col].str.replace(\"_x001D_\", \" \")\n",
    "            if 'ADP Position' in col:\n",
    "                summary_coc[col] = summary_coc[col].astype(str)\n",
    "        summary_coc = summary_coc.astype(object).where(pd.notna(summary_coc), None)\n",
    "        for col in summary_coc.loc[:, [col for col in summary_coc if 'Date' in col]].columns:\n",
    "            summary_coc[col] = pd.to_datetime(summary_coc[col], utc=False).apply(lambda x: x.replace(tzinfo=pytz.utc))\n",
    "        \n",
    "        channel_readings = channel_readings.astype(object).where(pd.notna(channel_readings), None)\n",
    "\n",
    "        channel_summary['File Source'] = filename\n",
    "        channel_readings['File Source'] = filename\n",
    "        summary_coc['File Source'] = filename\n",
    "        summary_coc.rename({'Flags':'Summary Flags'},axis=1,inplace=True)\n",
    "        channel_summary.rename({'Flags':'Channel Flags'},axis=1,inplace=True)\n",
    "        summary_coc = nmdx_file_parser.retrieveConsumableLots(summary_coc)\n",
    "        summary_coc = nmdx_file_parser.retrieveConsumableSerials(summary_coc)\n",
    "        summary_coc = nmdx_file_parser.retrieveConsumableExpiration(summary_coc)\n",
    "\n",
    "         \n",
    "        \n",
    "        \n",
    "\n",
    "        flat_data = summary_coc.join(channel_summary.loc[:, [x for x in channel_summary.columns if x not in summary_coc.columns]]).join(channel_readings.loc[:, [x for x in channel_readings.columns if x not in channel_summary.columns]])\n",
    "        self.getRawMinusBlankCheckReads(flat_data)\n",
    "        flat_data = self.channelParametersFlattener(flat_data)\n",
    "        ##Add Target Result / Localized Result columns if not in flat_data columns\n",
    "        if 'Localized Result' not in flat_data.columns:\n",
    "            flat_data['Localized Result'] = np.nan\n",
    "        \n",
    "        if 'Target Result' not in flat_data.columns:\n",
    "            flat_data['Target Result'] = np.nan\n",
    "\n",
    "        return flat_data.reset_index()\n",
    "\n",
    "    def getADFParameters(self, file):\n",
    "        \"\"\"\n",
    "        A Function used to get ADF Parameters from ADF Tabs\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file (str): Filepath of the NeuMoDx Raw Data File (xlsx) to read. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A Dictionary serializing the Parameters for ADF.\n",
    "        \"\"\"\n",
    "\n",
    "        file_data = pd.read_excel(io=file,sheet_name=None)\n",
    "\n",
    "        def get_adf_liquid_handling_parameters(adf_df):\n",
    "            \"\"\"\n",
    "            A function used to provide an order to for processing logic for the liquid handling parameters included in a NeuMoDx ADF Tab in Raw Data Export.\n",
    "            **Note this function is based on of Tab observed in 1.9.2.6 file format, and is is subject to break if References are changed.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            adf_df (pd.DataFrame):  A DataFrame Representation of an ADF Tab included in NeuMoDx Raw Data Exports.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            A Dictionary containing the Liquid Class Names and order for the TADM files used by a NeuMoDx ADFs.\n",
    "\n",
    "            \"\"\"\n",
    "            adf_df = adf_df.set_index('Key')\n",
    "            adf_dict = adf_df['Value'].to_dict()\n",
    "            ##Iterate through ADF_Dictionary to find Specimen Types included in ADF.\n",
    "            specimenTypes = []\n",
    "            for setting in [x for x in adf_dict if 'Specimen' in x and 'Liquid Class' in x]:\n",
    "                specimenType = setting.split(' -')[0].replace('Specimen ','')\n",
    "                if specimenType not in specimenTypes:\n",
    "                    specimenTypes.append(specimenType)\n",
    "\n",
    "            ##initialized empty dictionary describing \n",
    "            adf_liquid_class_orders = {}\n",
    "\n",
    "            ##Define Mapping for ADF parameters to lh order\n",
    "            lh_order = {'Buffer Liquid Class':1,\n",
    "                        'Sample Dispense Extraction Plate Liquid Class': 2,\n",
    "                        'Specimen Liquid Class':3,\n",
    "                        'Extraction Aspirate Extraction Plate Liquid Class':4,\n",
    "                        'Cartridge Dispense Empty Liquid Class':6,\n",
    "                        'Aspirate From Cartridge Liquid Class':8,\n",
    "                        'NeuMoDx Test Strip Liquid Class':9}\n",
    "\n",
    "            ##Determine which TADM File Name is associated with each Liquid Handling process.\n",
    "            for specimenType in specimenTypes:\n",
    "                ##Initialized liquid class order, include TADM files that are universal for each ADF (seal Checks).\n",
    "                liquid_class_order = {1:'_Aspirating', 2:'_Dispensing', 3:'_Aspirating', 4:'_Aspirating', 5:'NeuMoDx_HighVolumeFilter_Air_DispenseSurface_Aspirating', 6:'_Dispensing', 7:'NeuMoDx_LHPC1_StandardVolumeFilter_Air_DispenseSurface_Aspirating', 8:'_Aspirating', 9:'_Dispensing', 10:'', 11:'NeuMoDx_LHPC2_StandardVolumeFilter_Air_DispenseSurface_Aspirating', 12:'NMDX_LHPC2_CartBlow_TADM_Dispensing'}\n",
    "                \n",
    "                ##Iterate over items in adf_dict and map associated TADM File Name to liquid_class_order dictionary.\n",
    "                for liquid_class in [x for x in adf_dict if 'Specimen' in x and 'Liquid Class' in x and specimenType == x.split(' -')[0][-1*(len(specimenType)):]]:\n",
    "                    for process_type in lh_order:\n",
    "                        if process_type in liquid_class:\n",
    "                            base = liquid_class_order[lh_order[process_type]]\n",
    "                            liquid_class_order[lh_order[process_type]] = adf_dict[liquid_class]+base\n",
    "\n",
    "                liquid_class_order[10] = liquid_class_order[9].replace('_Dispensing', '_Aspirating')\n",
    "                liquid_class_order = dict((value, key) for (key, value) in liquid_class_order.items())\n",
    "                adf_liquid_class_orders[specimenType.replace(' ', '')] = liquid_class_order\n",
    "\n",
    "            return adf_liquid_class_orders\n",
    "        \n",
    "        for adf_sheet in [x for x in file_data if 'ADF' in x]:\n",
    "            \n",
    "            adf_label = adf_sheet.replace('ADF ', '')\n",
    "            if adf_label not in self.adf_TADM_order:\n",
    "                adf = file_data[adf_sheet]\n",
    "                self.adf_TADM_order[adf_label] = get_adf_liquid_handling_parameters(adf)\n",
    "\n",
    "class TadmHelper:\n",
    "\n",
    "    def __init__(self, raw_data):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_data pd.DataFrame: A Raw Data DataFrame in Flat format.\n",
    "        \"\"\"\n",
    "        self.tadm_data = pd.DataFrame()\n",
    "        self.raw_data = raw_data.copy()\n",
    "        self.channels = sorted(raw_data['Channel'].unique()) \n",
    "        self.raw_data.drop_duplicates(subset=['Test Guid', 'Replicate Number'], inplace=True)\n",
    "        self.raw_data_liquid_handle_processes = self.raw_data[['Test Guid', 'Replicate Number', 'Sample ID', 'Start Date/Time', 'LHPA Start Date Time', 'LHPB Start Date Time', 'LHPC Start Date Time', 'PCR Start Date Time', 'LHPA ADP Position', 'LHPB ADP Position', 'LHPC ADP Position', 'Assay Name', 'Assay Version', 'Test Specimen Type']]\n",
    "\n",
    "        self.processGroups = {}\n",
    "        for process in ['LHPA', 'LHPB', 'LHPC']:\n",
    "            self.processGroups[process] = self.raw_data[[process+' Start Date Time']].drop_duplicates([process+' Start Date Time']).dropna().sort_values(process+' Start Date Time').reset_index(drop=True)\n",
    "\n",
    "    def get_tadms(self, file_dir):\n",
    "        \"\"\"\n",
    "        A function used to prepare a the tadm_data by merging together data found within a directory.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_dir (str): Name of File Directory to Search for Files within.\n",
    "        \"\"\"\n",
    "        \n",
    "        attempt_id = uuid.uuid4()\n",
    "        files = [file_dir+'/'+x for x in os.listdir(file_dir)]\n",
    "        files\n",
    "        curvefile = [x for x in files if 'Curves' in x][0]\n",
    "        curves_df = pd.read_csv(curvefile).set_index(['CurveID', 'Sheet'])\n",
    "        pressurevalues = {}\n",
    "        ##Get unique pressurevalues\n",
    "        for pressurevalue in curves_df.index.unique(1):\n",
    "            df = pd.read_csv([x for x in files if pressurevalue in x][0])\n",
    "            df = df.set_index('Time').transpose()\n",
    "            df['Sheet'] = pressurevalue\n",
    "            df.index.names = ['CurveID']\n",
    "            df.reset_index(inplace=True)\n",
    "            df['CurveID'] = df['CurveID'].astype(int)\n",
    "            df.set_index(['CurveID','Sheet'], inplace=True)\n",
    "            df = df.join(curves_df)\n",
    "            pressurevalues[pressurevalue] = df\n",
    "\n",
    "        attempt_data = pd.concat([pressurevalues[df] for df in pressurevalues],axis=0).set_index(['LiquidClassName',\n",
    "                                                                                    'Volume',\n",
    "                                                                                    'StepType',\n",
    "                                                                                    'Channel',\n",
    "                                                                                    'Time',\n",
    "                                                                                    'StepNumber',\n",
    "                                                                                    'TadmMode',\n",
    "                                                                                    'TadmError'],append=True)\n",
    "        attempt_data['ParserID'] = attempt_id\n",
    "        attempt_data.set_index(['ParserID'],append=True)    \n",
    "        self.tadm_data = pd.concat([self.tadm_data, attempt_data])\n",
    "\n",
    "    def closest_match(self, sample, main_process, max_time_offset=30, min_time_offset=30, max_time_delta=400):\n",
    "       \n",
    "        def relabel_seal_check_retries(data, liquidclassname):\n",
    "\n",
    "            ##Get Minimimum Time for liquidclassname\n",
    "\n",
    "            min_time = data.loc[data['LiquidClassName']==liquidclassname, 'Time'].min()\n",
    "            itr = 0\n",
    "            \n",
    "            for step in data[data['LiquidClassName']==liquidclassname].index:\n",
    "                time_delta = data.loc[step, 'Time'] - min_time\n",
    "                if time_delta.total_seconds() < 60 and time_delta.total_seconds() != 0:\n",
    "                    itr = itr + 1\n",
    "                    data.loc[step, 'LiquidHandlingProcessOrder'] = str(data.loc[step, 'LiquidHandlingProcessOrder']) + \".\" + str(itr)\n",
    "\n",
    "            return data\n",
    "        \"\"\"\n",
    "        A function used to apply fuzzy logic to find tadms associated with a NeuMoDx Sample\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        Sample (pd.DataFrame):  a slice of one row of data from NeuMoDx Raw Data\n",
    "        main_process (str): Main Liquid Handling Process (LHPA, LHPB, LHPC) to use as time reference. \n",
    "        max_time_offset (int): An offset in seconds to apply to the maximum time bound applied to TADM search range.\n",
    "        min_time_offset (int): An offset in seconds to apply to the minimum time bound applied to TADM search range.\n",
    "        \"\"\"\n",
    "        def find_tadms(channel, repeat_offset=0, reschedule=0):\n",
    "            ##Get necessary info from sample.\n",
    "            test_guid = sample['Test Guid'].values[0]\n",
    "            self.find_tadms_current_test_guid = test_guid\n",
    "            rep_number = sample['Replicate Number'].values[0]\n",
    "            assay = sample['Assay Name'].values[0]+\",\"+sample['Assay Version'].values[0]\n",
    "            specimenType = sample['Test Specimen Type'].values[0]\n",
    "            \n",
    "            ##Convert Associated Start Date Time to be utc agnostic\n",
    "            sample[main_process+' Start Date Time'] = sample[main_process+' Start Date Time'].apply(lambda x: x.replace(tzinfo=pytz.utc))\n",
    "            \n",
    "            ##Get Time of associated Sample\n",
    "            time = sample[main_process+' Start Date Time'].astype('datetime64[ns]').values[0]\n",
    "            \n",
    "            ##Determine the processing group sample is associated with\n",
    "            processGroupTimes = self.processGroups[main_process]\n",
    "            processGroupTimes[main_process+\" Start Date Time\"] = processGroupTimes[main_process+\" Start Date Time\"].astype('datetime64[ns]')\n",
    "            processGroupTimes['Reference Time'] = time\n",
    "            processGroupTimes['Delta Time'] = abs(processGroupTimes[main_process+\" Start Date Time\"]-processGroupTimes['Reference Time'])\n",
    "            \n",
    "            ##Determine Minimum and Maximum Bounds for time allowed to search within.\n",
    "            minimum_time_bound_index = processGroupTimes.loc[processGroupTimes['Delta Time']==processGroupTimes['Delta Time'].min(), main_process+\" Start Date Time\"].index.values[0]\n",
    "\n",
    "            ##Apply a -1 run group offset in the case of a first time repeated sample.\n",
    "            minimum_time_bound_index = minimum_time_bound_index - repeat_offset\n",
    "\n",
    "            if minimum_time_bound_index < 0:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            ##Get Value of minimum_time_bound\n",
    "            minimum_time_bound = processGroupTimes.loc[minimum_time_bound_index, main_process+\" Start Date Time\"] - np.timedelta64(min_time_offset, 's')\n",
    "            time = processGroupTimes.loc[minimum_time_bound_index, main_process+\" Start Date Time\"]\n",
    "\n",
    "            if minimum_time_bound_index+1 < len(processGroupTimes):\n",
    "                maximum_time_bound = processGroupTimes.loc[minimum_time_bound_index+1, main_process+\" Start Date Time\"] + np.timedelta64(max_time_offset, 's')\n",
    "                if main_process == 'LHPB':\n",
    "                    maximum_time_bound = maximum_time_bound + np.timedelta64(60, 's')\n",
    "            else:\n",
    "                maximum_time_bound = minimum_time_bound + np.timedelta64(5, 'm')\n",
    "            \n",
    "            ##Filter TADM Reference to only be for the Channel and Time Range allowed to search within\n",
    "            tadm_reference_channel = self.tadm_data.reset_index(['Channel', 'LiquidClassName', 'StepType','Time'])\n",
    "            tadm_reference_channel['Time'] = tadm_reference_channel['Time'].astype('datetime64[ns]')\n",
    "            tadm_reference_channel = tadm_reference_channel[((tadm_reference_channel['Channel']==channel)&\n",
    "                                                            (tadm_reference_channel['Time']>minimum_time_bound)&\n",
    "                                                            (tadm_reference_channel['Time']<maximum_time_bound))]\n",
    "            \n",
    "\n",
    "            ##Determine Delta Time from Time observed for sample process\n",
    "            tadm_reference_channel['Reference Time'] = time\n",
    "            tadm_reference_channel['Delta Time'] = (tadm_reference_channel['Time']  - tadm_reference_channel['Reference Time']).astype('timedelta64[s]')\n",
    "            tadm_reference_channel = tadm_reference_channel[tadm_reference_channel['Delta Time']<max_time_delta]\n",
    "            \n",
    "            ##Add Test Guid / Replicate Number to TADM reference\n",
    "            tadm_reference_channel['Test Guid'] = test_guid\n",
    "            tadm_reference_channel['Replicate Number'] = rep_number\n",
    "            tadm_reference_channel = tadm_reference_channel.reset_index()[['ParserID', 'CurveID', 'Test Guid', 'Replicate Number', 'Time', 'Channel', 'Delta Time', 'LiquidClassName', 'StepType']]#.sort_values('Delta Time')\n",
    "            \n",
    "            ##Filter to make sure that we are only grabbing TADMs that we would expect based on process.\n",
    "            if main_process == 'LHPB':\n",
    "                tadm_reference_channel = tadm_reference_channel[((tadm_reference_channel['LiquidClassName'].str.contains(main_process))|(tadm_reference_channel['LiquidClassName'].str.contains('High')))]\n",
    "            else:\n",
    "                tadm_reference_channel = tadm_reference_channel[((tadm_reference_channel['LiquidClassName'].str.contains(main_process)))]\n",
    "\n",
    "            ##Add LiquidHandlingProcessOrder\n",
    "            tadm_reference_channel['LiquidHandlingProcessOrder'] = tadm_reference_channel['LiquidClassName']+'_'+tadm_reference_channel['StepType']\n",
    "            tadm_reference_channel['LiquidHandlingProcessOrder'] = tadm_reference_channel['LiquidHandlingProcessOrder'].str.replace('Reschedule_', '')\n",
    "            tadm_reference_channel['LiquidHandlingProcessOrder'] = tadm_reference_channel['LiquidHandlingProcessOrder'].replace(my_gui.myParser.adf_TADM_order[assay][specimenType])\n",
    "            tadm_reference_channel.sort_values(['Delta Time'],inplace=True)\n",
    "            \n",
    "            ##Logic to follow if executing under assumption that sample IS NOT rescheduled.\n",
    "            if reschedule == 0:\n",
    "                for idx in tadm_reference_channel.index:\n",
    "                    try:\n",
    "                        if tadm_reference_channel.loc[idx, 'LiquidHandlingProcessOrder']!=tadm_reference_channel['LiquidHandlingProcessOrder'].min():\n",
    "                            tadm_reference_channel.drop(idx, inplace=True)\n",
    "                        else: \n",
    "                            break\n",
    "                    except TypeError:\n",
    "                        print(\"error in comparing LiquidHandlingProcess Order for Test Guid: \"+test_guid)\n",
    "            \n",
    "            ##Logic to follow if executing under assumption that sample IS rescheduled.\n",
    "            else:\n",
    "                for idx in tadm_reference_channel.index:\n",
    "\n",
    "                    ##Do this to prevent Finder from picking up the LhpB Dispense that occured in the prior iteration\n",
    "                    ##This seems to happen because LhpB dispense occurs so far after the LhpB Start time that is \n",
    "                    if tadm_reference_channel.loc[idx, 'LiquidHandlingProcessOrder']==6 and tadm_reference_channel.loc[idx, 'Delta Time']<30:\n",
    "                        tadm_reference_channel.drop(idx, inplace=True)\n",
    "                    \n",
    "            ##Relabel TADM Retries for LHPB and LHPC Seal Checks  to make sure we do deleate\n",
    "            if main_process == 'LHPB':\n",
    "                tadm_reference_channel = relabel_seal_check_retries(tadm_reference_channel, 'NeuMoDx_HighVolumeFilter_Air_DispenseSurface')\n",
    "            \n",
    "            \n",
    "            if main_process == 'LHPC':\n",
    "                tadm_reference_channel = relabel_seal_check_retries(tadm_reference_channel, 'NeuMoDx_LHPC1_StandardVolumeFilter_Air_DispenseSurface')\n",
    "                tadm_reference_channel = relabel_seal_check_retries(tadm_reference_channel, 'NeuMoDx_LHPC2_StandardVolumeFilter_Air_DispenseSurface')\n",
    "\n",
    "            ##Drop any duplicates that may have been found, keep lowest time delta.\n",
    "            tadm_reference_channel.drop_duplicates(['LiquidHandlingProcessOrder','StepType'],keep='first',inplace=True)\n",
    "            tadm_reference_channel['MainProcess'] = main_process\n",
    "            tadm_reference_channel['ProcessStartTime'] = minimum_time_bound\n",
    "            return tadm_reference_channel\n",
    "\n",
    "\n",
    "        channel = sample.loc[:, main_process+' ADP Position'].values[0]\n",
    "        \n",
    "\n",
    "        ##Determine which Channel to work with and if a sample is a aborted, or repeated sample.\n",
    "        if pd.isnull(channel) or \"nan\" in channel:\n",
    "            return\n",
    "\n",
    "        elif \",\" in channel:\n",
    "            channel_1 = pd.to_numeric(channel[-1])\n",
    "            set1 = find_tadms(channel_1, reschedule=1)\n",
    "            channel_2 = pd.to_numeric(channel[0])\n",
    "            set2 = find_tadms(channel_2, repeat_offset=1)\n",
    "            return pd.concat([set1, set2],axis=0).drop_duplicates(['ParserID','CurveID'],keep='first')\n",
    "        else:\n",
    "            channel = pd.to_numeric(channel)\n",
    "            set1 = find_tadms(channel)\n",
    "            return set1\n",
    "\n",
    "    def tadm_hunter(self):\n",
    "        self.conversion_frame = pd.DataFrame()\n",
    "        limit = 100000 \n",
    "        iteration = 0\n",
    "        for id in self.raw_data_liquid_handle_processes.index.values:\n",
    "            if iteration < limit:\n",
    "                for process in ['LHPA', 'LHPB', 'LHPC']:\n",
    "                    self.conversion_frame = pd.concat([self.conversion_frame, self.closest_match(self.raw_data_liquid_handle_processes.loc[[id]], process)],axis=0)\n",
    "                iteration = iteration + 1 \n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        self.conversion_frame = self.conversion_frame[['Test Guid', 'Replicate Number', 'ParserID', 'CurveID', 'LiquidHandlingProcessOrder']].set_index(['Test Guid','Replicate Number', 'ParserID', 'CurveID', 'LiquidHandlingProcessOrder'])\n",
    "\n",
    "    def tadm_merger(self, include_XPCR_info=0, include_HM_info=0, include_TS_info=0, include_SP_info=0, include_ConsLot_info=0, include_ChannelResult_info=0):\n",
    "        raw_data_file_columns = ['Test Guid','Sample ID', 'Replicate Number','Overall Result','N500 Serial Number']\n",
    "        \n",
    "        if include_XPCR_info == 1:\n",
    "            raw_data_file_columns = raw_data_file_columns + ['XPCR Module Serial','XPCR Module Index','Pcr Cartridge Lane']\n",
    "\n",
    "        if include_HM_info == 1:\n",
    "            raw_data_file_columns = raw_data_file_columns +['Heater Module Serial','Heater Module Index','Capture Plate Well']\n",
    "\n",
    "        if include_TS_info == 1:\n",
    "            raw_data_file_columns = raw_data_file_columns +['Test Strip NeuMoDx Carrier', \n",
    "                                                            'Test Strip NeuMoDx Carrier Position', \n",
    "                                                            'Test Strip NeuMoDx Well',\n",
    "                                                            'Test Strip LDT Primer Probe Well',\n",
    "                                                            'Test Strip LDT Primer Carrier',\n",
    "                                                            'Test Strip LDT Primer Carrier Position',\n",
    "                                                            'Test Strip LDT Master Mix Carrier',\n",
    "                                                            'Test Strip LDT Master Mix Carrier Position',\n",
    "                                                            'Test Strip LDT Master Mix Well']\n",
    "\n",
    "        if include_SP_info == 1: \n",
    "            raw_data_file_columns = raw_data_file_columns + ['Sample Type', \n",
    "                                                            'Sample Specimen Type', \n",
    "                                                            'Test Specimen Type', \n",
    "                                                            'Specimen Tube Type', \n",
    "                                                            'Assay Name', \n",
    "                                                            'Result Code', \n",
    "                                                            'Status']\n",
    "        if include_ConsLot_info == 1:\n",
    "            raw_data_file_columns = raw_data_file_columns + ['LDT Test Strip Primer Probe Lot',\n",
    "                                                            'LDT Test Strip Master Mix Lot',\n",
    "                                                            'Pcr Cartridge Lot',\n",
    "                                                            'Capture Plate Lot',\n",
    "                                                            'Test Strip NeuMoDx Lot',\n",
    "                                                            'Buffer Lot',\n",
    "                                                            'Release Reagent Lot',\n",
    "                                                            'Wash Reagent Lot']\n",
    "        if include_ChannelResult_info == 1:\n",
    "            for channel in self.channels:\n",
    "                raw_data_file_columns = raw_data_file_columns+[channel + \" \" + x for x in ['Localized Result', 'Ct', 'End Point Fluorescence', 'EPR', 'Max Peak Height', 'Baseline Slope', 'Baseline Y Intercept', 'Blank Check - 1st 3 Reads'] if channel + \" \" + x in self.raw_data.columns]\n",
    "\n",
    "        raw_data_index = self.raw_data.loc[:, raw_data_file_columns].set_index(['Test Guid', 'Replicate Number'])\n",
    "        raw_data_index = raw_data_index.join(self.conversion_frame)\n",
    "        merged_data = raw_data_index.join(self.tadm_data.reset_index().set_index(['ParserID', 'CurveID']))\n",
    "        return merged_data.reset_index().set_index([x for x in self.tadm_data.index.names] + raw_data_file_columns)\n",
    "\n",
    "window_width = 1000\n",
    "window_height = 400\n",
    "windowsize = str(window_width)+\"x\"+str(window_height)\n",
    "root = Tk()\n",
    "root.title(\"TADM Matcher v0.3\")\n",
    "root.geometry(windowsize)\n",
    "my_gui = UI(root)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29e20157-3e6e-ed11-8625-b07b2500e41b'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gui.myTadmHelper.find_tadms_current_test_guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-11-27 11:30:16+0000', tz='UTC')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gui.raw_data['LHPA Start Date Time'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LHPA Start Date Time</th>\n",
       "      <th>LHPA ADP Position</th>\n",
       "      <th>LHPB Start Date Time</th>\n",
       "      <th>LHPC Start Date Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Guid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29e20157-3e6e-ed11-8625-b07b2500e41b</th>\n",
       "      <td>2022-11-27 11:30:16+00:00</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          LHPA Start Date Time  \\\n",
       "Test Guid                                                        \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b 2022-11-27 11:30:16+00:00   \n",
       "\n",
       "                                     LHPA ADP Position LHPB Start Date Time  \\\n",
       "Test Guid                                                                     \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b              1, 1                  NaT   \n",
       "\n",
       "                                     LHPC Start Date Time  \n",
       "Test Guid                                                  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  \n",
       "29e20157-3e6e-ed11-8625-b07b2500e41b                  NaT  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gui.raw_data.set_index('Test Guid').loc['29e20157-3e6e-ed11-8625-b07b2500e41b', ['LHPA Start Date Time', 'LHPA ADP Position', 'LHPB Start Date Time', 'LHPC Start Date Time']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ca8ae99df12afeecc637c741eef9d8fc79f3c8a2a662f29db32bfeb3c8d3abb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
